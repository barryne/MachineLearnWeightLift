sRes <- deviance(fit)
1- (sRes/sTot)
1-sRes/sTot
all.equal(0.2104629, summary(fit$r.squared))
summary(fit)$r.squared
(cor(galton$parent, galton$child))^2
cor(galton$parent,galton$child)^2
install.packages("caret")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
install.packages("kernLab")
install.packages("kernlab")
library("kernlab", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(spam)
str(spam)
?createDataPartition
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
?dim
str(training)
?train
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
modelFit <- train(type~., data=training, method="glm")
?install.packages
install.packages("e1071")
library("e1071", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
warnings()
modelFit
modelFit$finalModel
predictions <- predict(modelFit, newdata=testing)
predictions
confusionMatrix(predictions, testing$type)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library("kernlab", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain, ]
dim(training)
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
modelFit
modelFit$finalModel # we can see the final model that has been developed
predictions <- predict(modelFit, newdata=testing) # pass the model we built on the training data and pass it the test dataset
predictions # these are the set of predictions
# one way to evaluate whether the model predicts well or not is to
# use confusionMatrix function
# this evaluates the predictions against the actuals in the test dataset
confustionMatrix(predictions, testing$type)
# the output shows the accuracy, confidence intervals and sensitivity, specificity
confusionMatrix(predictions, testing$type)
# the output shows the accuracy, confidence intervals and sensitivity, specificity
# to do cross validation you can split the training set
# into a number of smaller datasets
set.seed(32323)
# create number of folds
# here we set to 10 folds
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=TRUE)
# you pass the createFolds function
# the outcome you want to split on, in this case the type variable in spam
# k, the number of folds we want to create
sapply(folds, length) # we can check the length of each fold in the folds list
# if we look at the first element of the folds list and the first 10 elements within that we get the following
folds[[1]][1:10]
# items are split up in order
########################################################################
########################################################################
# you can also have the createFolds function return the test set rather than the training set
set.seed(32323)
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=FALSE)
sapply(folds, length)
folds[[1]][1:10]
## you can create time slices with createTimeSlices
#e.g.
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme, initialWindow=20, horizon=10)
name(folds)
folds$train[[1]]
folds$test[[1]]
# folds contains train sets with 20 consecutive samples and
# test sets with 10 next consecutive samples
########################################################################
########################################################################
names(folds)
str(folds)
head(folds)
install.packages("ISLR")
library("ISLR", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x= training[, c("age", "education", "jobclass",)],
y= training$wage,
plot="pairs")
featurePlot(x= training[, c("age", "education", "jobclass")],
y= training$wage,
plot="pairs")
qplot(age, wage, data=training)
qplot(age, wage, colour=jobclass, data=training)
## add regression smoothers to do further eda
qq <- qplot(age, wage, colour=education, data=training)
qq + geom_smooth(method="lm", formula=y~x)
install.packages("Hmisc")
library("Hmisc", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
cutWage <- cut2(training$wage, g=3)
table(cutWage)
p1 <- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot"))
p1
p2 <- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol=2)
grid.arrange(p1, p2, ncol=2)
?grid.arrange()
install.packages("gridExtra")
library("gridExtra", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
grid.arrange(p1, p2, ncol=2)
#look at a table of the factorized version of the continuous variable of interest,
#wage in this case
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1)
prop.table(t1, 1)
qpot(wage, colour=education, data=training, geom="density")
qplot(wage, colour=education, data=training, geom="density")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
str(testing)
?concrete
featurePlot(x= training[, c("Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
"Superplasticizer", "CoarseAggregate", "FineAggregate"
"Age")],
y= training$CompressiveStrength,
plot="pairs")
featurePlot(x= training[, c("Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
"Superplasticizer", "CoarseAggregate", "FineAggregate",
"Age")],
y= training$CompressiveStrength,
plot="pairs")
qplot(Age, CompressiveStrength, data=training)
qplot(FlyAsh, CompressiveStrength, data=training)
qplot(FlyAsh, CompressiveStrength, colour=Age, data=training)
cutCS <- cut2(training$CompressiveStrength, g=4)
table(cutCS)
p1 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot"))
p1
p2 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p1, p2, ncol=2)
p3 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot"))
p3
p4 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p3, p4, ncol=2)
plot(cut2(mixtures$CompressiveStrength,m=20),pch=5)
plot(mixtures$CompressiveStrength,pch=1,col=cut2(mixtures$FlyAsh,m=20))
cutCS <- cut2(training$CompressiveStrength, g=10)
table(cutCS)
p1 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot"))
p1
p2 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p1, p2, ncol=2)
p3 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot"))
p3
p4 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p3, p4, ncol=2)
hist(training$Superplasticizer)
hist(training$Superplasticizer, 50)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(AlzheimerDisease)
summary(AlzheimerDisease)
str(adData)
?preProcess()
trainingIL <- training[ , grepl( "IL" , names( training ) ) ]
str(trainingIL)
trainingIL <- training[ , grepl( "^IL" , names( training ) ) ]
str(trainingIL)
trainingD <- training[, c("diagnosis")]
str(trainingD)
trainingDIL <- data.frame(trainingD, trainingIL)
str(trainingDIL)
trainingD <- training[, c("diagnosis")]
colnames(trainingD) <- "diagnosis"
trainingD <- as.dataframe(training[, c("diagnosis")])
trainingD <- as.data.frame(training[, c("diagnosis")])
class(trainingD)
str(trainingD)
colnames(trainingD) <- "diagnosis"
str(trainingD)
trainingDIL <- data.frame(trainingD, trainingIL)
str(trainingDIL)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProc <- preProcess(log10(trainingIL+1), method="pca", tresh=0.8)
preProc <- preProcess(trainingIL, method="pca", tresh=0.8)
summary(preProc)
str(preProc)
trainingILPC <- predict(preProc, trainingIL)
str(trainingILPC)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainingIL <- training[ , grepl( "^IL" , names( training ) ) ]
str(trainingIL)
preProc <- preProcess(log10(trainingIL+1), method="pca", thresh=0.8)
preProc <- preProcess(trainingIL, method="pca", thresh=0.8)
trainingILPC <- predict(preProc, trainingIL)
str(trainingILPC)
trainingD <- as.data.frame(training[, c("diagnosis")])
colnames(trainingD) <- "diagnosis"
trainingDIL <- data.frame(trainingD, trainingIL)
trainingDILPC <- data.frame(trainingD, trainingILPC)
## train the data using the first training set
modelFit1 <- train(diagnosis ~ ., method = "glm", data = trainingDIL)
predictions1 <- predict(modelFit1, newdata = testing)
C1 <- confusionMatrix(predictions1, testing$diagnosis)
## train the data using the second training set
modelFit2 <- train(diagnosis ~ ., method = "glm", data = trainingDILPC)
predictions2 <- predict(modelFit2, newdata = testing)
C1 <- confusionMatrix(predictions2, testing$diagnosis)
excludena95
dim(training)
The training dataset now contains 92 potential predictor variables and the `classe` variable which is the target variable.
summary(training)
library("ggplot2", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, destfile= "pml-training.csv", method="curl")
download.file(testURL, destfile= "pml-testing.csv", method="curl")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
dim(training)
dim(testing)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(training, nacalc)>=0.95))
excludena95
training <- training[,-which(names(training) %in% excludena95)]
dim(training)
summary(training)
featureplot(x=training[,c(1:93)], y=training$classe, plot="pairs")
featurePlot(x=training[,c(1:93)], y=training$classe, plot="pairs")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
#load the training and test datasets
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, destfile= "pml-training.csv", method="curl")
download.file(testURL, destfile= "pml-testing.csv", method="curl")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
dim(training)
dim(testing)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(training, nacalc)>=0.95))
excludena95
training <- training[,-which(names(training) %in% excludena95)]
dim(training)
table(training$classe)
featurePlot(x=training[,c(1:5)], y=training$classe, plots="pairs")
str(training)
?featurePlot
featurePlot(x=training[,c("x", "user_name")], y=training$classe, plots="pairs")
featurePlot(x=training[,c("X", "user_name")], y=training$classe, plots="pairs")
plot(training$magnet_forearm_z, training$classe)
for (i in 1:92) {
plot(training[,i], training$classe
}
for (i in 1:92) {
plot(training[,i], training$classe)
}
training <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
testing <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
dim(training)
dim(testing)
str(training)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(training, nacalc)>=0.95))
excludena95
training <- training[,-which(names(training) %in% excludena95)]
dim(training)
str(training)
plot(skewness_roll_forearm)
plot(training$skewness_roll_forearm)
head(training$skewness_roll_forearm)
tail(training$skewness_roll_forearm)
head(training$skewness_roll_forearm, 50)
summary(training)
training <- lapply(r[,8:92], as.numeric)
training <- lapply(training[,8:92], as.numeric)
str(trainin)
str(training)
str(training$skewness_yaw_forearm)
summary(training$skewness_yaw_forearm)
summary(training$skewness_roll_forearm)
#load the training and test datasets
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, destfile= "pml-training.csv", method="curl")
download.file(testURL, destfile= "pml-testing.csv", method="curl")
training <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
testing <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
dim(training)
dim(testing)
# check the detail of the training dataset for issues like NAs
str(training)
head(training[,160])
#convert sensor data from character to numeric
training <- lapply(training[,8:159], as.numeric)
str(training)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(training, nacalc)>=0.95))
excludena95
training <- training[,-which(names(training) %in% excludena95)]
dim(training)
#load the training and test datasets
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, destfile= "pml-training.csv", method="curl")
download.file(testURL, destfile= "pml-testing.csv", method="curl")
training <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
testing <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
dim(training)
dim(testing)
str(training)
#convert sensor data from character to numeric
training[,8:159] <- lapply(training[,8:159], as.numeric)
str(training)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(training, nacalc)>=0.95))
excludena95
training <- training[,-which(names(training) %in% excludena95)]
dim(training)
head(training[,60], 10)
intrain <- createDataPartition(y=wldata$classe, p = 0.8, list=FALSE)
training <- wldata[intrain, ]
testing <- wldata[-intrain, ]
dim(training)
dim(testing)
wldata <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
finaltest <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
dim(wldata)
dim(finaltest)
# check the detail of the weight lifting dataset for issues like NAs
str(wldata)
#convert sensor data from character to numeric
wldata[,8:159] <- lapply(wldata[,8:159], as.numeric)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(wldata, nacalc)>=0.95))
excludena95
wldata <- wldata[,-which(names(wldata) %in% excludena95)]
dim(wldata)
table(wldata$classe)
intrain <- createDataPartition(y=wldata$classe, p = 0.8, list=FALSE)
training <- wldata[intrain, ]
testing <- wldata[-intrain, ]
dim(training)
dim(testing)
model1 <- train(classe ~ ., method="rpart", data=training)
plotcp(model1)
model1
install.packages('caret', dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
model1 <- train(classe ~ ., method="rpart", data=training)
model1 <- train(classe ~ ., method="rf", data=training)
install.packages("randomForest")
install.packages("randomForest")
install.packages("randomForest")
install.packages("randomForest")
install.packages("caret")
library(caret)
library(randomForest)
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, destfile= "pml-training.csv", method="curl")
download.file(testURL, destfile= "pml-testing.csv", method="curl")
wldata <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
finaltest <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
dim(wldata)
dim(finaltest)
wldata[,8:159] <- lapply(wldata[,8:159], as.numeric)
nacalc <- function(x) {
sum(is.na(x))/length(x)
}
excludena95 <- names(which(sapply(wldata, nacalc)>=0.95))
excludena95
wldata <- wldata[,-which(names(wldata) %in% excludena95)]
str(wldata)
plot(wldata$new_window)
hist(wldata$new_window)
table(wldata$new_window)
hist(wldata$num_window)
wldatat <- wldata[,-c(1:5)]
str(wldatat)
wldatat <- NULL
wldata <- wldata[,-c(1:6)]
?nearZeroVar
nzv <- nearZeroVar(wldata, saveMetrics = TRUE)
str(nzv)
table(nzv$nzv)
str(wldata)
intrain <- createDataPartition(y=wldata$classe, p = 0.8, list=FALSE)
training <- wldata[intrain, ]
testing <- wldata[-intrain, ]
dim(training)
dim(testing)
model1 <- train(classe ~ ., method="rf", data=training)
install.packages("e1071")
library(e1071)
model1 <- train(classe ~ ., method="rf", data=training)
model1 <- train(classe ~ ., method="rpart", data=training)
warnings()
model1 <- train(classe ~ train[,-54], method="rpart", data=training)
model1 <- train(classe ~ train[,c(1:53)], method="rpart", data=training)
model1 <- train(classe ~ training[,c(1:53)], method="rpart", data=training)
model1 <- train(classe ~ ., method="rpart", data=training)
modfit<- randomForest(classe ~., data = training)
str(training)
?train()
wldata$classe <- as.factor(wldata$classe)
str(wldata)
table(wldata$classe)
intrain <- createDataPartition(y=wldata$classe, p = 0.8, list=FALSE)
training <- wldata[intrain, ]
testing <- wldata[-intrain, ]
dim(training)
dim(testing)
model1 <- train(classe ~ ., method="rpart", data=training)
print(model1$finalModel)
plot(model1$finalModel)
plot(model1$finalModel, main="Classification Tree")
text(model1$finalModel, use.n=TRUE, all=TRUE)
plot(model1$finalModel, main="Classification Tree")
text(model1$finalModel, use.n=TRUE, all=TRUE, cex=0.5)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(model1$finalModel)
model2 <- train(classe ~ ., method="rf", data=training)
pred1 <- predict(model1, testing)
pred2 <- predict(model2, testing)
table(pred1, testing$classe)
table(pred2, testing$classe)
confusionMatrix(pred1, testing$classe)
confusionMatrix(pred2, testing$classe)
getwd()
setwd(/Users/marybarry/nadine/DataScience/MachineLearn/Predictions")
setwd(/Users/marybarry/nadine/DataScience/MachineLearn/Predictions)
setwd("/Users/marybarry/nadine/DataScience/MachineLearn/Predictions")
getwd()
predfinal <- predict(model2, finaltest)
predfinal
str(predfinal)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
predfinal <- as.character(predfinal)
pml_write_files(predfinal)
The files were submitted to https://class.coursera.org/predmachlearn-013/assignment which confirmed 100% accuracy of the model on these 20 finaltest cases.
