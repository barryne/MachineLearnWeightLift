addd2 <- function(x, y) {
x + y
}
addd2(3, 5)
}
install.packages('knitr')
library("knitr")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
xyplot(weight ~ Time | Diet, BodyWeight)
head(diet)
head(Diet)
head(bodyweight)
head(BodyWeight)
head(BodyWeight$Diet)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
print(p)
library(ggplot2)
packages("ggplot2")
install.package("ggplot2")
install.packages("ggplot2")
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = .~factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = .~Month)
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
?sqe()
data(mtcars)
round(t.test(mtcars$mpg)$conf.int)
head(mtcars)
cylfac <- as.factor(cyl)
fit <- lm(mpg ~ cylfac + wt, data=mtcars)
mtcarsnk <- mtcars
mtcarsnk$cylfac <- as.factor(mtcars.cyl)
fit <- lm(mpg ~ cylfac + wt, data=mtcarsnk)
mtcarsnk <- mtcars
mtcarsnk$cylfac <- as.factor(mtcars$cyl)
fit <- lm(mpg ~ cylfac + wt, data=mtcarsnk)
fut
fit
summary(fit)
fit2 <- lm(mpg ~ cylfac, data=mtcarsnk)
summart(fit2)
summary(fit2)
fit3 <- lm(mpg ~ cylfac * wt, mtcarsnk)
summary(fit3)
fit11 <- lm(mpg ~ factor(cyl) + wt, mtcars)
fit12 <- lm(mpg ~ factor(cyl) * wt, mtcars)
fit11
fit12
fit11$coeff
plot(fit11)
anova(fit11, fit12)
fit5 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
fit6 <- lm(mpg ~ I(wt * 1) + factor(cyl), data = mtcars)
fit5
fit6
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
?dfbetas
fitbeta <- lm(formula = y ~ x)
dfbetas(fitbeta)
hatvalues(fitbeta)
fitbeta2 <- lm(x~y)
hatvalues(fitbeta)
?datasets
?mtcars
detach("package:datasets", unload=TRUE)
library("MASS", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
?shuttle
dataset(shuttle)
data(shuttle)
str(shuttle)
logShuttle <- glm(shuttle$use ~ shuttle$wind, family="binomial")
summary(logShuttle)
exp(-0.25131)
exp(-0.03181)
plot(shuttle$wind, logShuttle$fitted, pch=19, col="blue")
exp(-.25131+(-0.03181*logShuttle$fitted)) / 1+exp(-.25131+(-0.03181*logShuttle$fitted))
exp(-.25131+(-0.03181*shuttle$wind)) / 1+exp(-.25131+(-0.03181*shuttle$wind))
as.character(shuttle$use)
shuttle$binuse[shuttle$use == "auto"] <- "1"
shuttle$binuse[shuttle$use == "noauto"] <- "0"
as.numeric(shuttle$binuse)
logShuttle2 <- glm(shuttle$use ~ shuttle$wind, family="binomial")
summary(logShuttle2)
as.character(shuttle$use)
shuttle$binuse[shuttle$use == "auto"] <- "1"
shuttle$binuse[shuttle$use == "noauto"] <- "0"
as.numeric(shuttle$binuse)
as.character(shuttle$wind)
shuttle$binwind[shuttle$wind == "head"] <- "1"
shuttle$binwind[shuttle$wind == "tail"] <- "0"
as.numeric(shuttle$binwind)
logShuttle2 <- glm(shuttle$binuse ~ shuttle$binwind, family="binomial")
summary(logShuttle2)
exp(-0.03181)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn, family="binomial")
summary(logShuttle3)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind * shuttle$magn, family="binomial")
summary(logShuttle3)
str(shuttle$magn)
### Q2
as.numeric(shuttle$magn)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn, family="binomial")
summary(logShuttle3)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn)
str(shuttle)
as.numeric(shuttle$magn)
as.numeric(shuttle$binwind)
shuttle$binuse <- as.numeric(shuttle$binuse)
as.character(shuttle$wind)
shuttle$binwind[shuttle$wind == "head"] <- "1"
shuttle$binwind[shuttle$wind == "tail"] <- "0"
shuttle$binwind <- as.numeric(shuttle$binwind)
logShuttle2 <- glm(shuttle$binuse ~ shuttle$binwind, family="binomial")
summary(logShuttle2)
exp(-0.03181)
### Q2
shuttle$magn <- $as.numeric(shuttle$magn)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn)
summary(logShuttle3)
### Q2
shuttle$magn <- as.numeric(shuttle$magn)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn)
summary(logShuttle3)
### Q2
shuttle$magn <- as.numeric(shuttle$magn)
logShuttle3 <- glm(shuttle$binuse ~ shuttle$binwind + shuttle$magn)
summary(logShuttle3)
exp(-0.007812)
logShuttle4 <- glm(1-shuttle$binuse ~ shuttle$binwind, family="binomial")
summary(logShuttle4)
logShuttle2 <- glm(shuttle$binuse ~ shuttle$binwind, family="binomial")
summary(logShuttle2)
data(InsectSprays)
str(InsectSprays)
pois <- glm(InsectSprays$count ~ InsectSprays$spray, family = poisson)
pois
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knot <- 0
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "blue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
library("swirl", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent, galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3,
| col='red')
abline(regrline, lwd=3,  col='red')
summary(regrline)
cor(gpa_nor, gch_nor)
l_not <- (gch_not ~ gpa_nor)
l_not <- (gch_nor ~ gpa_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child ~ parent)
fit <- lm(child ~ parent, data=galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- est(ols.slope)
varEst <- est(ols.ic+ols.slope*galton$parent)
info()
varEst <-
varEst <- est(x)
varEst <- est(ols.slope+sltweak[n],ols.ic+ictweak[n])
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, varRes+varEst)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
fit <- lm(child ~ parent, data=galton)
fit <- lm(child ~ parent, galton)
sqrt((sum(fit$residuals^2))/(n-2))
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum(((gatlon$child)-mu)^2)
sTot <- sum(((galton$child)-mu)^2)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit$residuals)
sRes <- deviance(fit)
1- (sRes/sTot)
1-sRes/sTot
all.equal(0.2104629, summary(fit$r.squared))
summary(fit)$r.squared
(cor(galton$parent, galton$child))^2
cor(galton$parent,galton$child)^2
install.packages("caret")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
install.packages("kernLab")
install.packages("kernlab")
library("kernlab", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(spam)
str(spam)
?createDataPartition
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
?dim
str(training)
?train
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
modelFit <- train(type~., data=training, method="glm")
?install.packages
install.packages("e1071")
library("e1071", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
warnings()
modelFit
modelFit$finalModel
predictions <- predict(modelFit, newdata=testing)
predictions
confusionMatrix(predictions, testing$type)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library("kernlab", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain, ]
dim(training)
set.seed(32343)
modelFit <- train(type~., data=training, method="glm")
modelFit
modelFit$finalModel # we can see the final model that has been developed
predictions <- predict(modelFit, newdata=testing) # pass the model we built on the training data and pass it the test dataset
predictions # these are the set of predictions
# one way to evaluate whether the model predicts well or not is to
# use confusionMatrix function
# this evaluates the predictions against the actuals in the test dataset
confustionMatrix(predictions, testing$type)
# the output shows the accuracy, confidence intervals and sensitivity, specificity
confusionMatrix(predictions, testing$type)
# the output shows the accuracy, confidence intervals and sensitivity, specificity
# to do cross validation you can split the training set
# into a number of smaller datasets
set.seed(32323)
# create number of folds
# here we set to 10 folds
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=TRUE)
# you pass the createFolds function
# the outcome you want to split on, in this case the type variable in spam
# k, the number of folds we want to create
sapply(folds, length) # we can check the length of each fold in the folds list
# if we look at the first element of the folds list and the first 10 elements within that we get the following
folds[[1]][1:10]
# items are split up in order
########################################################################
########################################################################
# you can also have the createFolds function return the test set rather than the training set
set.seed(32323)
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=FALSE)
sapply(folds, length)
folds[[1]][1:10]
## you can create time slices with createTimeSlices
#e.g.
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y=tme, initialWindow=20, horizon=10)
name(folds)
folds$train[[1]]
folds$test[[1]]
# folds contains train sets with 20 consecutive samples and
# test sets with 10 next consecutive samples
########################################################################
########################################################################
names(folds)
str(folds)
head(folds)
install.packages("ISLR")
library("ISLR", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x= training[, c("age", "education", "jobclass",)],
y= training$wage,
plot="pairs")
featurePlot(x= training[, c("age", "education", "jobclass")],
y= training$wage,
plot="pairs")
qplot(age, wage, data=training)
qplot(age, wage, colour=jobclass, data=training)
## add regression smoothers to do further eda
qq <- qplot(age, wage, colour=education, data=training)
qq + geom_smooth(method="lm", formula=y~x)
install.packages("Hmisc")
library("Hmisc", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
cutWage <- cut2(training$wage, g=3)
table(cutWage)
p1 <- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot"))
p1
p2 <- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol=2)
grid.arrange(p1, p2, ncol=2)
?grid.arrange()
install.packages("gridExtra")
library("gridExtra", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
grid.arrange(p1, p2, ncol=2)
#look at a table of the factorized version of the continuous variable of interest,
#wage in this case
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1)
prop.table(t1, 1)
qpot(wage, colour=education, data=training, geom="density")
qplot(wage, colour=education, data=training, geom="density")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
str(testing)
?concrete
featurePlot(x= training[, c("Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
"Superplasticizer", "CoarseAggregate", "FineAggregate"
"Age")],
y= training$CompressiveStrength,
plot="pairs")
featurePlot(x= training[, c("Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
"Superplasticizer", "CoarseAggregate", "FineAggregate",
"Age")],
y= training$CompressiveStrength,
plot="pairs")
qplot(Age, CompressiveStrength, data=training)
qplot(FlyAsh, CompressiveStrength, data=training)
qplot(FlyAsh, CompressiveStrength, colour=Age, data=training)
cutCS <- cut2(training$CompressiveStrength, g=4)
table(cutCS)
p1 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot"))
p1
p2 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p1, p2, ncol=2)
p3 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot"))
p3
p4 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p3, p4, ncol=2)
plot(cut2(mixtures$CompressiveStrength,m=20),pch=5)
plot(mixtures$CompressiveStrength,pch=1,col=cut2(mixtures$FlyAsh,m=20))
cutCS <- cut2(training$CompressiveStrength, g=10)
table(cutCS)
p1 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot"))
p1
p2 <- qplot(cutCS, Age, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p1, p2, ncol=2)
p3 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot"))
p3
p4 <- qplot(cutCS, FlyAsh, data=training, fill=cutCS, geom=c("boxplot", "jitter"))
#install the gridExtra package
grid.arrange(p3, p4, ncol=2)
hist(training$Superplasticizer)
hist(training$Superplasticizer, 50)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(AlzheimerDisease)
summary(AlzheimerDisease)
str(adData)
?preProcess()
trainingIL <- training[ , grepl( "IL" , names( training ) ) ]
str(trainingIL)
trainingIL <- training[ , grepl( "^IL" , names( training ) ) ]
str(trainingIL)
trainingD <- training[, c("diagnosis")]
str(trainingD)
trainingDIL <- data.frame(trainingD, trainingIL)
str(trainingDIL)
trainingD <- training[, c("diagnosis")]
colnames(trainingD) <- "diagnosis"
trainingD <- as.dataframe(training[, c("diagnosis")])
trainingD <- as.data.frame(training[, c("diagnosis")])
class(trainingD)
str(trainingD)
colnames(trainingD) <- "diagnosis"
str(trainingD)
trainingDIL <- data.frame(trainingD, trainingIL)
str(trainingDIL)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProc <- preProcess(log10(trainingIL+1), method="pca", tresh=0.8)
preProc <- preProcess(trainingIL, method="pca", tresh=0.8)
summary(preProc)
str(preProc)
trainingILPC <- predict(preProc, trainingIL)
str(trainingILPC)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainingIL <- training[ , grepl( "^IL" , names( training ) ) ]
str(trainingIL)
preProc <- preProcess(log10(trainingIL+1), method="pca", thresh=0.8)
preProc <- preProcess(trainingIL, method="pca", thresh=0.8)
trainingILPC <- predict(preProc, trainingIL)
str(trainingILPC)
trainingD <- as.data.frame(training[, c("diagnosis")])
colnames(trainingD) <- "diagnosis"
trainingDIL <- data.frame(trainingD, trainingIL)
trainingDILPC <- data.frame(trainingD, trainingILPC)
## train the data using the first training set
modelFit1 <- train(diagnosis ~ ., method = "glm", data = trainingDIL)
predictions1 <- predict(modelFit1, newdata = testing)
C1 <- confusionMatrix(predictions1, testing$diagnosis)
## train the data using the second training set
modelFit2 <- train(diagnosis ~ ., method = "glm", data = trainingDILPC)
predictions2 <- predict(modelFit2, newdata = testing)
C1 <- confusionMatrix(predictions2, testing$diagnosis)
getwd()
setwd("/Users/marybarry/nadine/DataScience/MachineLearn")
getwd
getwd()
head(testing)
